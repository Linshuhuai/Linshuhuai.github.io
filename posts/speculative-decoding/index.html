<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Speculative Decoding | Shuhuai&#39;Log</title>
<meta name="keywords" content="LLM, ML Systems">
<meta name="description" content="Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. Speculative decoding is a pivotal technique to accelerate the inference of Large Language Models (LLMs) by employing a smaller draft model to predict the target model’s outputs. This article provides a brief introduction to speculative decoding and discusses early attempts in this field.
Basics Autoregressive Decoding Transformer-based LLMs typically make generations in an autoregressive manner.">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://linshuhuai.github.io/posts/speculative-decoding/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://linshuhuai.github.io/img/tree-512.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://linshuhuai.github.io/img/tree-16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://linshuhuai.github.io/img/tree-32.png">
<link rel="apple-touch-icon" href="https://linshuhuai.github.io/img/tree-512.png">
<link rel="mask-icon" href="https://linshuhuai.github.io/img/tree-512.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://linshuhuai.github.io/posts/speculative-decoding/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


<meta property="og:title" content="Speculative Decoding" />
<meta property="og:description" content="Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. Speculative decoding is a pivotal technique to accelerate the inference of Large Language Models (LLMs) by employing a smaller draft model to predict the target model’s outputs. This article provides a brief introduction to speculative decoding and discusses early attempts in this field.
Basics Autoregressive Decoding Transformer-based LLMs typically make generations in an autoregressive manner." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://linshuhuai.github.io/posts/speculative-decoding/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-28T15:14:31-08:00" />
<meta property="article:modified_time" content="2024-02-28T15:14:31-08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Speculative Decoding"/>
<meta name="twitter:description" content="Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. Speculative decoding is a pivotal technique to accelerate the inference of Large Language Models (LLMs) by employing a smaller draft model to predict the target model’s outputs. This article provides a brief introduction to speculative decoding and discusses early attempts in this field.
Basics Autoregressive Decoding Transformer-based LLMs typically make generations in an autoregressive manner."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://linshuhuai.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Speculative Decoding",
      "item": "https://linshuhuai.github.io/posts/speculative-decoding/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Speculative Decoding",
  "name": "Speculative Decoding",
  "description": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. Speculative decoding is a pivotal technique to accelerate the inference of Large Language Models (LLMs) by employing a smaller draft model to predict the target model’s outputs. This article provides a brief introduction to speculative decoding and discusses early attempts in this field.\nBasics Autoregressive Decoding Transformer-based LLMs typically make generations in an autoregressive manner.",
  "keywords": [
    "LLM", "ML Systems"
  ],
  "articleBody": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. Speculative decoding is a pivotal technique to accelerate the inference of Large Language Models (LLMs) by employing a smaller draft model to predict the target model’s outputs. This article provides a brief introduction to speculative decoding and discusses early attempts in this field.\nBasics Autoregressive Decoding Transformer-based LLMs typically make generations in an autoregressive manner. Given an input sequence $x_{1}, …, x_{t}$, an autoregressive language model $M_q$ generates the next token according to:\n$$ x_{t+1} ∼ q_{n+1} = M_q (x|x_{\\leq t+1}), \\tag{1} $$\nwhere $q$ is the conditional probability distribution calculated by $M_q$ and $x_{t+1}$ denotes the next token sampled from $q_{n+1}$. A detailed process is shown in Algorithm 1.\nWhile the standard autoregressive decoding offers desirable generation quality, it is bounded by memory bandwidth, resulting in low utilization of modern accelerators. In this process, each memory-bound LLM call (i.e., an LLM forward step) produces merely a single token for the entire sequence, making the whole generation inefficient and time-consuming.\nSpeculative Decoding Speculative decoding is founded upon two key observations about LLM inference:\nMany easy tokens can be predicted with less computational overhead (e.g., using a smaller model). LLM inference is highly memory bandwidth bound (Patterson et al, 2004) with the main latency bottleneck arising from memory reads/writes of LLM parameters rather than arithmetic computations. Drawing on these observations, speculative decoding adapts the concept of speculative execution to focus LLMs’ computational efforts on the validation of pre-drafted tokens, substantially diminishing the need for frequent memory operations of LLM parameters, thereby improving inference efficiency.\nSpeculative decoding is a Draft-then-Verify decoding paradigm in which, at each decoding step, it first efficiently drafts multiple future tokens and then verifies all these tokens in parallel using the target LLM to speed up inference. Algorithm 2 formulates a detailed Speculative Decoding process.\nFig. 1. In contrast to autoregressive decoding (left) that generates sequentially, speculative decoding (right) first efficiently drafts multiple tokens and then verifies them in parallel using the target LLM. (Image source: Xia et al. 2024)\nAs shown in Figure 1, in each decoding step, speculative decoding first efficiently drafts multiple tokens as speculation of future decoding steps of the target LLM and then utilizes the LLM to verify all drafted tokens in parallel. Only those tokens that meet the LLM’s verification criterion are accepted as final outputs to guarantee generation quality.\nDrafting At each decoding step, speculative decoding first efficiently drafts multiple future tokens, as a speculation of the target LLM’s output. Formally, given an input sequence $x_{1}, …, x_{t}$ and the target LLM $M_q$, this paradigm employs an efficient drafter model $M_p$ (e.g., a smaller LM) to decode the next $K$ drafted tokens:\n$$ p_1, \\ldots, p_K = \\text{DRAFT}(x_{\\leq t}, M_p), \\tag{2} $$ $$ \\tilde{x}_i \\sim p_i, i = 1, \\ldots, K, $$\nwhere $DRAFT(.)$ denotes various drafting strategies that we will discuss later, $p$ is the conditional probability distribution calculated by $M_p$, and $\\tilde{x}_i$ denotes the drafted token sampled from $p_i$.\nVerification Subsequently, these drafted tokens are verified by the target LLM $M_q$ in parallel. Formally, given the input sequence $x_{1}, …, x_{t}$ and the draft $ \\tilde{x}_1, …, \\tilde{x}_K $, speculative decoding utilizes $M_q$ to compute $K+1$ probability distributions simultaneously:\n$$ q_i = M_q(x | x_{\\leq t},\\tilde{x}_{\u003c i}), i = 1, \\ldots, K+1. \\tag{3} $$\nThen, each drafted token $\\tilde{x}_i$ is verified by a specific criterion $ \\text{VERIFY} (\\tilde{x}_i, p_i, q_i) $. Only those tokens that meet the criterion are selected as final outputs, ensuring quality consistent with the target LLM’s standards. Otherwise, the first drafted token $\\tilde{x}_c$ that fails the verification will be corrected by the strategy $CORRECT (p_c, q_c)$.\nAll drafted tokens after position $c$ will be discarded, to guarantee the high quality of the final outputs. If all tokens pass verification, an additional token $x_{t+K+1}$ will be sampled from $q_{K+1}$ as Eq. (1).\nThe drafting and verification substeps will be iterated until the termination condition is met, i.e., the [EOS] token is decoded or the sentence reaches the maximal length.\nOverview This is an overview of the field’s early research stages and its development over time. (until 2023 Sept.)\nFig. 2. Timeline illustrating the evolution of Speculative Decoding. (Image source: Xia et al. 2024)\nStern et al. 2018 introduced Blockwise Decoding, an approach that incorporates extra feedforward neural (FFN) heads atop the Transformer decoder, enabling the simultaneous drafting of multiple tokens per step. These tokens are then verified by the original LLM in parallel, ensuring that the outputs align with those of the original LLM.\nAfter 2022, speculative decoding was formally introduced as a general decoding paradigm to accelerate LLM inference and garnered widespread attention. The following sections provide a detailed categorization and discussion of these studies and subsequent research.\nDrafting Strategies As a vital component of speculative decoding, the drafting process has a crucial impact on the speedup of the paradigm. The impact is determined by two key factors: the speculation accuracy of the drafter $M_p$, measured by the average number of accepted tokens per step, and the drafting latency.\nIndependent Drafting 1. Fine-tuned Drafter: SpecDec, SpecInfer\nTo strike a balance between speculation accuracy and efficiency, SpecDec first proposed utilizing an independent model for drafting. Specifically, it employed a specialized NonAutoregressive Transformer that drafts multiple tokens simultaneously per step. This model has a deep-shallow encoder-decoder architecture to run efficiently. Despite its strengths, SpecDec requires training a drafter model from scratch, which demands an increased computational budget.\n2. Tuning-free Drafter: Speculative Decoding, SpecTr\nConsidering the available models in existing LLM series (e.g., OPT and LLaMA), a more straightforward and efficient approach is directly employing a small LM from the same series as the drafter to accelerate the inference of its larger counterparts. For instance, Leviathan et al. 2023 utilized T5-small as the drafter, to accelerate the inference of T5-XXL. These off-the-shelf small LMs do not require additional training or any modification on model architectures, facilitating the quick adoption of speculative decoding. Moreover, since models in the same series share tokenizers, pretraining corpora, and sumilar training processes, they inherently have an alignment in prediction behaviors.\nSelf-Drafting When no smaller counterparts of the LLM are available, e.g. LLaMA-7B, it takes a lot of effort to train or identify a drafter model that closely aligns with the target LLM. Furthermore, integrating two distinct models within a single system introduces additional computational complexity, particularly in distributed settings. To address the above issues, numerous studies have suggested leveraging the target LLM itself for efficient drafting.\n1. FFN Heads: Blockwise, Mesdusa\nParticularly, Blockwise Decoding and Medusa incorporated FFN heads atop the Transformer decoder, enabling the parallel token generation per step. Compared with external drafters, these lightweight heads reduce extra computational overhead and are friendly to distributed inference.\n2. Early Exiting: PPD, Self-Speculative\nAnother line of research has explored the potential of early exiting and layer skipping within the target LLM for drafting. For instance, PPD introduced additional subprocesses that exit early during the current decoding step, thereby initiating the drafting of future tokens in advance. Similarly, Self-Speculative proposed to adaptively skip several intermediate layers during inference to draft efficiently.\n3. Mask-Predict: Parallel Decoding, Lookahead Decoding, PaSS\nIn contrast to prior work that focused on extending model architectures or altering the inference process, Parallel Decoding introduced a simple drafting strategy that directly appends multiple [PAD] tokens to the end of the input prompt to enable parallel generation. However, this approach deviates from LLMs’ autoregressive pretraining pattern, leading to suboptimal drafting quality. To tackle this, Lookahead Decoding proposed transforming low-quality drafts into multiple n-grams to improve the speculation accuracy; PaSS introduced multiple learnable [LA] tokens and finetuned these token embeddings on a small training dataset to enhance the parallel decoding performance.\nVerification Strategies In each decoding step, the drafted tokens are verified in parallel to ensure the outputs align with the target LLM.\nGreedy Decoding Early attempts at speculative decoding focused on the verification criterion supporting greedy decoding, which guarantees that the outputs are exactly the same as the greedy decoding results of the target LLM. Formally, given the input sequence $x_1, …, x_t$, the drafted tokens $\\tilde{x}_1, …, \\tilde{x}_K$, and the computed probability distributions $p_1, …, p_K$, $q_1, …, q_K$ as obtained from Eq. (2) and (3), respectively, the verification criterion on the ith drafted token is formulated as\n$$ \\tilde{x}_i = \\arg\\max q_i, \\tag{4} $$\nwhere $i = 1, …, K$. The first position $c$ that the drafted token exc fails the verification denotes the bifurcation position. The output token at this position $x_{t+c}$ will be adjusted by the correction strategy, which simply replaces the drafted token with the LLM’s top-1 prediction:\n$$ x_{t+c} ← \\arg\\max q_c. \\tag{5} $$\nThe verification criterion of greedy decoding is straightforward and clear. Thus, multiple subsequent studies have adopted this criterion to demonstrate the efficacy of their methodologies(Parallel Decoding; PPD; SPEED; Self-Speculative; Lookahead Decoding). However, the strict matching requirement of this criterion often results in the rejection of high-quality drafted tokens, simply because they differ from the top-1 predictions of the target LLM, thereby constraining the speedup of the paradigm.\nTo tackle this problem, multiple studies have proposed various approximate verification criteria (Blockwise Decoding; SpecDec; BiLD). Compared with the lossless criterion, these methods slightly relax the matching requirement to trust the drafts more, leading to higher acceptance of drafted tokens. For instance, SpecDec only requires the drafted tokens to fall in top-k candidates of the target LLM; BiLD proposed a rollback criterion that only rejects drafted tokens when the number of consecutive mismatch tokens exceeds a fixed threshold.\nNucleus Sampling Following Stern et al. 2019, subsequent work extended Speculative Decoding to support nucleus sampling (Leviathan et al. 2023; Chen et al. 2023a), accelerating the target LLM’s inference without changing its output distribution.\nFormally, given the initial sequence $x_1, …, x_t$, the drafted tokens $\\tilde{x}_1, …, \\tilde{x}_K$, and the computed distributions $p_1, …, p_K$, $q_1, …, q_K$, the verification criterion on the 𝑖-th drafted token is\n$$ r \u003c min(1, \\frac{q_i(\\tilde{x}_i)}{p_i(\\tilde{x}_i)}), r∼U[0, 1], \\tag{6} $$\nwhere $r$ denotes a random number drawn from a uniform distribution $U [0, 1]$; $q_i(\\tilde{x}_i)$ and $p_i(\\tilde{x}_i)$ are the probability of $\\tilde{x}_i$ according $M_q$ and $M_p$, respectively; and $i = 1, …, K$. In other words, this criterion accepts the token $\\tilde{x}_i$ if $q_i(\\tilde{x}_i) ≥ p_i(\\tilde{x}_i)$, and in case $q_i(\\tilde{x}_i) \u003c p_i(\\tilde{x}_i)$ it rejects the token with probability $1 - \\frac{q_i(\\tilde{x}_i)}{p_i(\\tilde{x}_i)}$. The correction strategy resamples the output token at the bifurcation position $c$ from an adjusted distribution:\n$$ x_{t+c} \\sim \\text{norm}(\\max(0, q_c - p_c)). \\tag{7} $$\nIn addition to the strict requirement, some work has also explored approximate strategies to improve the token acceptance rate (Leviathan et al. 2023; Zhou et al. 2023). For instance, Leviathan et al. proposed multiplying $p_i(\\tilde{x}_i)$ in Eq. (6) by a lenience parameter $l \\in [0, 1]$ to slightly relax the criterion.\nToken Tree Verification SpecInfer is a system that improves the end-to-end latency and computational efficiency of LLM serving with tree-based speculative inference and verification (Figure 3).\nFig. 3. Comparing the incremental decoding approach used by existing LLM serving systems, the sequence-based speculative inference approach, and the tree-based speculative inference approach used by SpecInfer. (Image source: Miao et al. 2024)\nA key insight behind SpecInfer is to simultaneously consider a diversity of speculation candidates to maximize speculative performance. These candidates are organized as a token tree, whose nodes each represent a potential token sequence. The correctness of all candidate token sequences is verified against the LLM in parallel, which significantly increases the number of generated tokens in an LLM decoding step.\nSpecInfer introduces two methods for constructing token trees: expansion- and merge-based tree constructions.\n1. Expansion-basedtoken treeconstruction\nOne approach to creating a token tree involves deriving multiple tokens from a small speculative model (SSM) within a single decoding step. Compared to only using the top-1 token from an SSM, using the top-𝑘 (e.g. 𝑘=5) tokens can increase the success rate for decoding.\nDirectly selecting the top-𝑘 tokens at each step leads to an exponential increase in the number of potential token sequences, which substantially elevates inference latency and memory overhead. Consequently, this paper adopts a static strategy that expands the token tree following a preset expansion configuration represented as a vector of integers ⟨$𝑘_1, 𝑘_2, …, 𝑘_𝑚$⟩, where $m$ denotes the maximum number ofspeculative decoding steps, and 𝑘𝑖 indicates the number oftokens to expand for each token in the 𝑖-th step. For example, Figure 4 illustrates the expansion configuration ⟨2, 2, 1⟩, leading to four token sequences.\nFig. 4. Illustration of token tree expansion. (Image source: Miao et al. 2024)\n2. Merge-based token tree construction\nIn addition to using a single SSM, SpecInfer can also combine multiple SSMs, and to jointly predict an LLM’s output.\nSpecInfer first fine-tunes one SSM at a time to the fullest and marks all prompt samples where the SSM and LLM generate identical subsequent tokens. Next, SpecInfer filters all marked prompt samples and uses all remaining samples in the corpus to fine-tune the next SSM to the fullest. By repeating this process for every SSM in the pool, SpecInfer obtains a diverse set of SSMs whose aggregated output largely overlaps with the LLM’s output on the training corpus.\nIn the case where multiple SSMs are employed, the output of each SSM is considered as a token tree, and SpecInfer performs token tree merge to aggregate all speculated tokens in a single tree structure. Figure 4 shows the token tree derived by merging three sequences of tokens. Each token sequence is identified by a node in the merged token tree.\nNote that, in addition to boosting, there are several other ensemble learning methods (e.g., voting, bagging, and stacking)[4] that can be used to combine the outputs from multiple SSMs, and we leave the exploration as future work.\nConclusion Fig. 5. Summary of various speculative decoding methods. (Image source: Miao et al. 2024)\nFigure 5 shows a summary of various speculative decoding methods. “Independent-D” and “Self-D” denote independent drafting and self-drafting, respectively. “Greedy”, “Nucleus”, and “Token Tree” denote whether the method supports greedy decoding, nucleus sampling, and token tree verification, respectively. Here is a list of the most representative target LLMs for each method and the speedups in the original paper (if reported), which is obtained with a batch size of 1.\nThe goal of this article is to clarify the current research landscape and provide insights into future research directions :)\n*The majority of this article is excerpted from the paper “Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding” [1].\nReferences [1] Xia et al. “Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding”. arXiv:2401.07851 [cs.CL]\n[2] Patterson et al. “Latency Lags Bandwith”. Commun. ACM, 47(10):71–75.\n[3] Miao et al. “SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification”. arXiv:2305.09781 [cs.CL]\n[4] Ganaie et al. “Ensemble deep learning: A review. Engineering Applications of Artificial Intelligence” arXiv:2104.02395 [cs.LG]\n[5] Stern et al. “Blockwise Parallel Decoding for Deep Autoregressive Models” NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 10107–10116.\n[6] Zhang et al. “OPT: Open Pre-trained Transformer Language Models” arXiv:2205.01068 [cs.CL]\n[7] Touvron et al. “LLaMA: Open and Efficient Foundation Language Models” arXiv:2302.13971 [cs.CL]\n[8] Cai et al. “Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads” arXiv:2401.10774 [cs.LG]\n[9] Yang et al. “Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding” arXiv:2307.05908 [cs.CL]\n[10] Zhang et al. “Draft \u0026 Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding” arXiv:2309.08168 [cs.CL]\n[11] Santilli et al. “Accelerating Transformer Inference for Translation via Parallel Decoding” In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12336–12355, Toronto, Canada. Association for Computational Linguistics.\n[12] Fu et al. “Break the Sequential Dependency of LLM Inference Using Lookahead Decoding” arXiv:2402.02057 [cs.LG]\n[13] Monea et al. “PaSS: Parallel Speculative Sampling” arXiv:2311.13581 [cs.CL]\n[14] Hopper et al. “SPEED: Speculative Pipelined Execution for Efficient Decoding” arXiv:2310.12072 [cs.CL]\n[15] Xia et al. “Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation” arXiv:2203.16487 [cs.CL]\n[16] Leviathan et al. “Fast Inference from Transformers via Speculative Decoding” Proceedings of the 40th International Conference on Machine Learning, PMLR 202:19274-19286, 2023.\n[17] Chen et al. “Accelerating Large Language Model Decoding with Speculative Sampling” arXiv:2302.01318 [cs.CL]\n[18] Zhou et al. “DistillSpec: Improving Speculative Decoding via Knowledge Distillation” arXiv:2310.08461 [cs.CL]\n",
  "wordCount" : "2731",
  "inLanguage": "en",
  "datePublished": "2024-02-28T15:14:31-08:00",
  "dateModified": "2024-02-28T15:14:31-08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://linshuhuai.github.io/posts/speculative-decoding/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shuhuai'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://linshuhuai.github.io/img/tree-512.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://linshuhuai.github.io/" accesskey="h" title="Shuhuai&#39;Log (Alt + H)">Shuhuai&#39;Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://linshuhuai.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://linshuhuai.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Speculative Decoding
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2024-02-28 15:14:31 -0800 PST'>February 28, 2024</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Theme PaperMod&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/speculative-decoding/index.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#basics" aria-label="Basics">Basics</a><ul>
                        
                <li>
                    <a href="#autoregressive-decoding" aria-label="Autoregressive Decoding">Autoregressive Decoding</a></li>
                <li>
                    <a href="#speculative-decoding" aria-label="Speculative Decoding">Speculative Decoding</a></li>
                <li>
                    <a href="#drafting" aria-label="Drafting">Drafting</a></li>
                <li>
                    <a href="#verification" aria-label="Verification">Verification</a></li></ul>
                </li>
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#drafting-strategies" aria-label="Drafting Strategies">Drafting Strategies</a><ul>
                        
                <li>
                    <a href="#independent-drafting" aria-label="Independent Drafting">Independent Drafting</a></li>
                <li>
                    <a href="#self-drafting" aria-label="Self-Drafting">Self-Drafting</a></li></ul>
                </li>
                <li>
                    <a href="#verification-strategies" aria-label="Verification Strategies">Verification Strategies</a><ul>
                        
                <li>
                    <a href="#greedy-decoding" aria-label="Greedy Decoding">Greedy Decoding</a></li>
                <li>
                    <a href="#nucleus-sampling" aria-label="Nucleus Sampling">Nucleus Sampling</a></li>
                <li>
                    <a href="#token-tree-verification" aria-label="Token Tree Verification">Token Tree Verification</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. Speculative decoding is a pivotal technique to accelerate the inference of Large Language Models (LLMs) by employing a smaller draft model to predict the target model’s outputs. This article provides a brief introduction to speculative decoding and discusses early attempts in this field.</p>
<h2 id="basics">Basics<a hidden class="anchor" aria-hidden="true" href="#basics">#</a></h2>
<h3 id="autoregressive-decoding">Autoregressive Decoding<a hidden class="anchor" aria-hidden="true" href="#autoregressive-decoding">#</a></h3>
<p>Transformer-based LLMs typically make generations in an autoregressive manner. Given an input sequence $x_{1}, &hellip;, x_{t}$, an autoregressive language model $M_q$ generates the next token according to:</p>
<p>$$
x_{t+1} ∼ q_{n+1} = M_q (x|x_{\leq t+1}), \tag{1}
$$</p>
<p>where $q$ is the conditional probability distribution calculated by $M_q$ and $x_{t+1}$ denotes the next token sampled from $q_{n+1}$. A detailed process is shown in Algorithm 1.</p>
<figure class="align-center ">
    <img loading="lazy" src="pics/autoregressive.jpg#center" width="60%"/> 
</figure>

<p>While the standard autoregressive decoding offers desirable generation quality, it is bounded by memory bandwidth, resulting in low utilization of modern accelerators. In this process, each memory-bound LLM call (i.e., an LLM forward step) produces merely a single token for the entire sequence, making the whole generation inefficient and time-consuming.</p>
<h3 id="speculative-decoding">Speculative Decoding<a hidden class="anchor" aria-hidden="true" href="#speculative-decoding">#</a></h3>
<p>Speculative decoding is founded upon two key observations about LLM inference:</p>
<ol>
<li>Many easy tokens can be predicted with less computational overhead (e.g., using a smaller model).</li>
<li>LLM inference is highly memory bandwidth bound (<a href="https://dl.acm.org/doi/10.1145/1022594.1022596">Patterson et al, 2004</a>) with the main latency bottleneck arising from memory reads/writes of LLM parameters rather than arithmetic computations.</li>
</ol>
<p>Drawing on these observations, speculative decoding adapts the concept of speculative execution to focus LLMs’ computational efforts on the validation of pre-drafted tokens, substantially <strong>diminishing the need for frequent memory operations of LLM parameters</strong>, thereby improving inference efficiency.</p>
<figure class="align-center ">
    <img loading="lazy" src="pics/speculative.jpg#center" width="60%"/> 
</figure>

<p>Speculative decoding is a <strong>Draft-then-Verify</strong> decoding paradigm in which, at each decoding step, it first efficiently drafts multiple future tokens and then verifies all these tokens in parallel using the target LLM to speed up inference. Algorithm 2 formulates a detailed Speculative Decoding process.</p>
<figure class="align-center ">
    <img loading="lazy" src="pics/intro.jpg#center"
         alt="Fig. 1. In contrast to autoregressive decoding (left) that generates sequentially, speculative decoding (right) first efficiently drafts multiple tokens and then verifies them in parallel using the target LLM. (Image source: Xia et al. 2024)" width="75%"/> <figcaption>
            <p>Fig. 1. In contrast to autoregressive decoding (left) that generates sequentially, speculative decoding (right) first efficiently drafts multiple tokens and then verifies them in parallel using the target LLM. (Image source: <a href="https://arxiv.org/abs/2401.07851">Xia et al. 2024</a>)</p>
        </figcaption>
</figure>

<p>As shown in Figure 1, in each decoding step, speculative decoding first efficiently drafts multiple tokens as speculation of future decoding steps of the target LLM and then utilizes the LLM to verify all drafted tokens in parallel. Only those tokens that meet the LLM’s verification criterion are accepted as final outputs to guarantee generation quality.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="drafting">Drafting<a hidden class="anchor" aria-hidden="true" href="#drafting">#</a></h3>
<p>At each decoding step, speculative decoding first efficiently drafts multiple future tokens, as a speculation of the target LLM&rsquo;s output. Formally, given an input sequence $x_{1}, &hellip;, x_{t}$ and the target LLM $M_q$, this paradigm employs an efficient drafter model $M_p$ (e.g., a smaller LM) to decode the next $K$ drafted tokens:</p>
<p>$$
p_1, \ldots, p_K = \text{DRAFT}(x_{\leq t}, M_p), \tag{2}
$$
$$
\tilde{x}_i \sim p_i, i = 1, \ldots, K,
$$</p>
<p>where $DRAFT(.)$ denotes various drafting strategies that we will discuss later, $p$ is the conditional probability distribution calculated by $M_p$, and $\tilde{x}_i$ denotes the drafted token sampled from $p_i$.</p>
<h3 id="verification">Verification<a hidden class="anchor" aria-hidden="true" href="#verification">#</a></h3>
<p>Subsequently, these drafted tokens are verified by the target LLM $M_q$ in parallel. Formally, given the input sequence $x_{1}, &hellip;, x_{t}$ and the draft $ \tilde{x}_1, &hellip;, \tilde{x}_K $, speculative decoding utilizes $M_q$ to compute $K+1$ probability distributions simultaneously:</p>
<p>$$
q_i = M_q(x | x_{\leq t},\tilde{x}_{&lt; i}), i = 1, \ldots, K+1. \tag{3}
$$</p>
<p>Then, each drafted token $\tilde{x}_i$ is verified by a specific criterion $ \text{VERIFY} (\tilde{x}_i, p_i, q_i) $.
Only those tokens that meet the criterion are selected as final outputs, ensuring quality consistent with the target LLM’s standards. Otherwise, the first drafted token $\tilde{x}_c$ that fails the verification will be corrected by the strategy $CORRECT (p_c, q_c)$.</p>
<p>All drafted tokens after position $c$ will be discarded, to guarantee the high quality of the final outputs. If all tokens pass verification, an additional token $x_{t+K+1}$ will be sampled from $q_{K+1}$ as Eq. (1).</p>
<p>The drafting and verification substeps will be iterated until the termination condition is met, i.e., the [EOS] token is decoded or the sentence reaches the maximal length.</p>
<h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>This is an overview of the field&rsquo;s early research stages and its development over time. (until 2023 Sept.)</p>
<figure class="align-center ">
    <img loading="lazy" src="pics/timeline.jpg#center"
         alt="Fig. 2. Timeline illustrating the evolution of Speculative Decoding. (Image source: Xia et al. 2024)" width="100%"/> <figcaption>
            <p>Fig. 2. Timeline illustrating the evolution of Speculative Decoding. (Image source: <a href="https://arxiv.org/abs/2401.07851">Xia et al. 2024</a>)</p>
        </figcaption>
</figure>

<p><a href="https://arxiv.org/abs/1811.03115">Stern et al. 2018</a> introduced Blockwise Decoding, an approach that incorporates extra feedforward neural (FFN) heads atop the Transformer decoder, enabling the simultaneous drafting of multiple tokens per step. These tokens are then verified by the original LLM in parallel, ensuring that the outputs align with those of the original LLM.</p>
<p>After 2022, speculative decoding was formally introduced as a general decoding paradigm to accelerate LLM inference and garnered widespread attention. The following sections provide a detailed categorization and discussion of these studies and subsequent research.</p>
<h2 id="drafting-strategies">Drafting Strategies<a hidden class="anchor" aria-hidden="true" href="#drafting-strategies">#</a></h2>
<p>As a vital component of speculative decoding, the drafting process has a crucial impact on the speedup of the paradigm. The impact is determined by two key factors: the speculation accuracy of the drafter $M_p$, measured by the average number of accepted tokens per step, and the drafting latency.</p>
<h3 id="independent-drafting">Independent Drafting<a hidden class="anchor" aria-hidden="true" href="#independent-drafting">#</a></h3>
<p><strong>1. Fine-tuned Drafter: SpecDec, SpecInfer</strong></p>
<p>To strike a balance between speculation accuracy and efficiency, <a href="https://arxiv.org/abs/2401.07851">SpecDec</a> first proposed utilizing an independent model for drafting. Specifically, it employed a specialized NonAutoregressive Transformer that drafts multiple tokens simultaneously per step. This model has a deep-shallow encoder-decoder architecture to run efficiently. Despite its strengths, <a href="https://arxiv.org/abs/2401.07851">SpecDec</a> requires training a drafter model from scratch, which demands an increased computational budget.</p>
<p><strong>2. Tuning-free Drafter: Speculative Decoding, SpecTr</strong></p>
<p>Considering the available models in existing LLM series (e.g., <a href="https://arxiv.org/abs/2205.01068">OPT</a> and <a href="https://arxiv.org/abs/2302.13971">LLaMA</a>), a more straightforward and efficient approach is directly employing a small LM from the same series as the drafter to accelerate the inference of its larger counterparts. For instance, <a href="https://proceedings.mlr.press/v202/leviathan23a.html">Leviathan et al. 2023</a> utilized T5-small as the drafter, to accelerate the inference of T5-XXL. These off-the-shelf small LMs do not require additional training or any modification on model architectures, facilitating the quick adoption of speculative decoding. Moreover, since models in the same series share tokenizers, pretraining corpora, and sumilar training processes, they inherently have an alignment in prediction behaviors.</p>
<h3 id="self-drafting">Self-Drafting<a hidden class="anchor" aria-hidden="true" href="#self-drafting">#</a></h3>
<p>When no smaller counterparts of the LLM are available, e.g. <a href="https://arxiv.org/abs/2302.13971">LLaMA-7B</a>, it takes a lot of effort to train or identify a drafter model that closely aligns with the target LLM. Furthermore, integrating two distinct models within a single system introduces additional computational complexity, particularly in distributed settings. To address the above issues, numerous studies have suggested leveraging the target LLM itself for efficient drafting.</p>
<p><strong>1. FFN Heads: Blockwise, Mesdusa</strong></p>
<p>Particularly, <a href="https://arxiv.org/abs/1811.03115">Blockwise Decoding</a> and <a href="https://arxiv.org/abs/2401.10774">Medusa</a> incorporated FFN heads atop the Transformer decoder, enabling the parallel token generation per step. Compared with external drafters, these lightweight heads reduce extra computational overhead and are friendly to distributed inference.</p>
<p><strong>2. Early Exiting: PPD, Self-Speculative</strong></p>
<p>Another line of research has explored the potential of early exiting and layer skipping within the target LLM for drafting. For instance, <a href="https://arxiv.org/abs/2307.05908">PPD</a> introduced additional subprocesses that exit early during the current decoding step, thereby initiating the drafting of future tokens in advance. Similarly, <a href="https://arxiv.org/abs/2309.08168">Self-Speculative</a> proposed to adaptively skip several intermediate layers during inference to draft efficiently.</p>
<p><strong>3. Mask-Predict: Parallel Decoding, Lookahead Decoding, PaSS</strong></p>
<p>In contrast to prior work that focused on extending model architectures or altering the inference process, <a href="https://aclanthology.org/2023.acl-long.689/">Parallel Decoding</a> introduced a simple drafting strategy that directly appends multiple [PAD] tokens to the end of the input prompt to enable parallel generation. However, this approach deviates from LLMs’ autoregressive pretraining pattern, leading to suboptimal drafting quality. To tackle this, <a href="https://arxiv.org/abs/2402.02057">Lookahead Decoding</a> proposed transforming low-quality drafts into multiple n-grams to improve the speculation accuracy; <a href="https://arxiv.org/abs/2311.13581">PaSS</a> introduced multiple learnable [LA] tokens and finetuned these token embeddings on a small training dataset to enhance the parallel decoding performance.</p>
<h2 id="verification-strategies">Verification Strategies<a hidden class="anchor" aria-hidden="true" href="#verification-strategies">#</a></h2>
<p>In each decoding step, the drafted tokens are verified in parallel to ensure the outputs align with the target LLM.</p>
<h3 id="greedy-decoding">Greedy Decoding<a hidden class="anchor" aria-hidden="true" href="#greedy-decoding">#</a></h3>
<p>Early attempts at speculative decoding focused on the verification criterion supporting greedy decoding, which guarantees that the outputs are exactly the same as the greedy decoding results of the target LLM. Formally, given the input sequence $x_1, &hellip;, x_t$, the drafted tokens $\tilde{x}_1, &hellip;, \tilde{x}_K$, and the computed probability distributions $p_1, &hellip;, p_K$, $q_1, &hellip;, q_K$ as obtained from Eq. (2) and (3), respectively, the verification criterion on the ith drafted token is formulated as</p>
<p>$$
\tilde{x}_i = \arg\max  q_i, \tag{4}
$$</p>
<p>where $i = 1, &hellip;, K$. The first position $c$ that the drafted token exc fails the verification denotes the <em>bifurcation</em> position. The output token at this position $x_{t+c}$ will be adjusted by the correction strategy, which simply replaces the drafted token with the LLM’s top-1 prediction:</p>
<p>$$
x_{t+c} ← \arg\max q_c. \tag{5}
$$</p>
<p>The verification criterion of greedy decoding is straightforward and clear. Thus, multiple subsequent studies have adopted this criterion to demonstrate the efficacy of their methodologies(<a href="https://aclanthology.org/2023.acl-long.689/">Parallel Decoding</a>; <a href="https://arxiv.org/abs/2307.05908">PPD</a>; <a href="https://arxiv.org/abs/2310.12072">SPEED</a>; <a href="https://arxiv.org/abs/2309.08168">Self-Speculative</a>; <a href="https://arxiv.org/abs/2402.02057">Lookahead Decoding</a>). However, the strict matching requirement of this criterion often results in the rejection of high-quality drafted tokens, simply because they differ from the top-1 predictions of the target LLM, thereby constraining the speedup of the paradigm.</p>
<p>To tackle this problem, multiple studies have proposed various approximate verification criteria (<a href="https://arxiv.org/abs/1811.03115%5B">Blockwise Decoding</a>; <a href="https://arxiv.org/abs/2203.16487">SpecDec</a>; <a href="https://arxiv.org/abs/2302.07863">BiLD</a>). Compared with the lossless criterion, these methods slightly relax the matching requirement to trust the drafts more, leading to higher acceptance of drafted tokens. For instance, <a href="https://arxiv.org/abs/2203.16487">SpecDec</a> only requires the drafted tokens to fall in top-k candidates of the target LLM; <a href="https://arxiv.org/abs/2302.07863">BiLD</a> proposed a rollback criterion that only rejects drafted tokens when the number of consecutive mismatch tokens exceeds a fixed threshold.</p>
<h3 id="nucleus-sampling">Nucleus Sampling<a hidden class="anchor" aria-hidden="true" href="#nucleus-sampling">#</a></h3>
<p>Following <a href="http://proceedings.mlr.press/v97/stern19a.html">Stern et al. 2019</a>, subsequent work extended Speculative Decoding to support nucleus sampling (<a href="https://proceedings.mlr.press/v202/leviathan23a.html">Leviathan et al. 2023</a>; <a href="https://arxiv.org/abs/2302.01318">Chen et al. 2023a</a>), accelerating the target LLM’s inference without changing its output distribution.</p>
<p>Formally, given the initial sequence $x_1, &hellip;, x_t$, the drafted tokens $\tilde{x}_1, &hellip;, \tilde{x}_K$, and the computed distributions $p_1, &hellip;, p_K$, $q_1, &hellip;, q_K$, the verification criterion on the 𝑖-th drafted token is</p>
<p>$$
r &lt; min(1, \frac{q_i(\tilde{x}_i)}{p_i(\tilde{x}_i)}), r∼U[0, 1], \tag{6}
$$</p>
<p>where $r$ denotes a random number drawn from a uniform distribution $U [0, 1]$; $q_i(\tilde{x}_i)$ and $p_i(\tilde{x}_i)$ are the probability of $\tilde{x}_i$ according $M_q$ and $M_p$, respectively; and $i = 1, &hellip;, K$. In other words, this criterion accepts the token $\tilde{x}_i$ if $q_i(\tilde{x}_i) ≥ p_i(\tilde{x}_i)$, and in case $q_i(\tilde{x}_i) &lt; p_i(\tilde{x}_i)$ it rejects the token with probability $1 - \frac{q_i(\tilde{x}_i)}{p_i(\tilde{x}_i)}$. The correction strategy resamples the output token at the bifurcation position $c$ from an adjusted distribution:</p>
<p>$$
x_{t+c} \sim \text{norm}(\max(0, q_c - p_c)). \tag{7}
$$</p>
<p>In addition to the strict requirement, some work has also explored approximate strategies to improve the token acceptance rate (<a href="https://proceedings.mlr.press/v202/leviathan23a.html">Leviathan et al. 2023</a>; <a href="https://arxiv.org/abs/2310.08461">Zhou et al. 2023</a>). For instance, Leviathan et al. proposed multiplying $p_i(\tilde{x}_i)$ in Eq. (6) by a lenience parameter $l \in [0, 1]$ to slightly relax the criterion.</p>
<h3 id="token-tree-verification">Token Tree Verification<a hidden class="anchor" aria-hidden="true" href="#token-tree-verification">#</a></h3>
<p><a href="https://arxiv.org/abs/2305.09781">SpecInfer</a> is a system that improves the end-to-end latency and computational efficiency of LLM serving with tree-based speculative inference and verification (Figure 3).</p>
<figure class="align-center ">
    <img loading="lazy" src="pics/decoding-ways.jpg#center"
         alt="Fig. 3. Comparing the incremental decoding approach used by existing LLM serving systems, the sequence-based speculative inference approach, and the tree-based speculative inference approach used by SpecInfer. (Image source: Miao et al. 2024)" width="75%"/> <figcaption>
            <p>Fig. 3. Comparing the incremental decoding approach used by existing LLM serving systems, the sequence-based speculative inference approach, and the tree-based speculative inference approach used by SpecInfer. (Image source: <a href="https://arxiv.org/abs/2305.09781">Miao et al. 2024</a>)</p>
        </figcaption>
</figure>

<p>A key insight behind SpecInfer is to simultaneously consider a diversity of speculation candidates to maximize speculative performance. These candidates are organized as a <strong>token tree</strong>, whose nodes each represent a potential token sequence. The correctness of all candidate token sequences is verified against the LLM in parallel, which significantly increases the number of generated tokens in an LLM decoding step.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>SpecInfer introduces two methods for constructing token trees: expansion- and merge-based tree constructions.</p>
<p><strong>1. Expansion-basedtoken treeconstruction</strong></p>
<p>One approach to creating a token tree involves deriving multiple tokens from a small speculative model (SSM) within a single decoding step. Compared to only using the top-1 token from an SSM, using the top-𝑘 (e.g. 𝑘=5) tokens can increase the success rate for decoding.</p>
<p>Directly selecting the top-𝑘 tokens at each step leads to an exponential increase in the number of potential token sequences, which substantially elevates inference latency and memory overhead. Consequently, this paper adopts a static strategy that expands the token tree following a preset expansion configuration represented as a vector of integers ⟨$𝑘_1, 𝑘_2, &hellip;, 𝑘_𝑚$⟩, where $m$ denotes the maximum number ofspeculative decoding steps, and 𝑘𝑖 indicates the number oftokens to expand for each token in the 𝑖-th step. For example, Figure 4 illustrates the expansion configuration ⟨2, 2, 1⟩, leading to four token sequences.</p>
<figure class="align-center ">
    <img loading="lazy" src="pics/token-tree.jpg#center"
         alt="Fig. 4. Illustration of token tree expansion. (Image source: Miao et al. 2024)" width="80%"/> <figcaption>
            <p>Fig. 4. Illustration of token tree expansion. (Image source: <a href="https://arxiv.org/abs/2305.09781">Miao et al. 2024</a>)</p>
        </figcaption>
</figure>

<p><strong>2. Merge-based token tree construction</strong></p>
<p>In addition to using a single SSM, SpecInfer can also combine multiple SSMs, and to jointly predict an LLM’s output.</p>
<p>SpecInfer first fine-tunes one SSM at a time to the fullest and marks all prompt samples where the SSM and LLM generate identical subsequent tokens. Next, SpecInfer filters all marked prompt samples and uses all remaining samples in the corpus to fine-tune the next SSM to the fullest. By repeating this process for every SSM in the pool, SpecInfer obtains a diverse set of SSMs whose aggregated output largely overlaps with the LLM’s output on the training corpus.</p>
<p>In the case where multiple SSMs are employed, the output of each SSM is considered as a token tree, and SpecInfer performs token tree merge to aggregate all speculated tokens in a single tree structure. Figure 4 shows the token tree derived by merging three sequences of tokens. Each token sequence is identified by a node in the merged token tree.</p>
<p>Note that, in addition to boosting, there are several other ensemble learning methods (e.g., voting, bagging, and stacking)[4] that can be used to combine the outputs from multiple SSMs, and we leave the exploration as future work.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<figure class="align-center ">
    <img loading="lazy" src="pics/summary.jpg#center"
         alt="Fig. 5. Summary of various speculative decoding methods. (Image source: Miao et al. 2024)" width="100%"/> <figcaption>
            <p>Fig. 5. Summary of various speculative decoding methods. (Image source: <a href="https://arxiv.org/abs/2305.09781">Miao et al. 2024</a>)</p>
        </figcaption>
</figure>

<p>Figure 5 shows a summary of various speculative decoding methods. “Independent-D” and “Self-D” denote independent drafting and self-drafting, respectively. “Greedy”, “Nucleus”, and “Token Tree” denote whether the method supports greedy decoding, nucleus sampling, and token tree verification, respectively. Here is a list of the most representative target LLMs for each method and the speedups in the original paper (if reported), which is obtained with a batch size of 1.</p>
<p>The goal of this article is to clarify the current research landscape and provide insights into future research directions :)</p>
<p>*The majority of this article is excerpted from the paper &ldquo;Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding&rdquo; [1].</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Xia et al. <a href="https://arxiv.org/abs/2401.07851">&ldquo;Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding&rdquo;</a>. arXiv:2401.07851 [cs.CL]</p>
<p>[2] Patterson et al. <a href="https://arxiv.org/abs/2401.07851">&ldquo;Latency Lags Bandwith&rdquo;</a>. Commun. ACM, 47(10):71–75.</p>
<p>[3] Miao et al. <a href="https://arxiv.org/abs/2305.09781">&ldquo;SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification&rdquo;</a>. arXiv:2305.09781 [cs.CL]</p>
<p>[4] Ganaie et al. <a href="https://arxiv.org/abs/2104.02395">&ldquo;Ensemble deep learning: A review. Engineering Applications of Artificial Intelligence&rdquo;</a> arXiv:2104.02395 [cs.LG]</p>
<p>[5] Stern et al. <a href="https://arxiv.org/abs/1811.03115">&ldquo;Blockwise Parallel Decoding for Deep Autoregressive Models&rdquo;</a> NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 10107–10116.</p>
<p>[6] Zhang et al. <a href="https://arxiv.org/abs/2205.01068">&ldquo;OPT: Open Pre-trained Transformer Language Models&rdquo;</a> arXiv:2205.01068 [cs.CL]</p>
<p>[7] Touvron et al. <a href="https://arxiv.org/abs/2302.13971">&ldquo;LLaMA: Open and Efficient Foundation Language Models&rdquo;</a> arXiv:2302.13971 [cs.CL]</p>
<p>[8] Cai et al. <a href="https://arxiv.org/abs/2401.10774">&ldquo;Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads&rdquo;</a> arXiv:2401.10774 [cs.LG]</p>
<p>[9] Yang et al. <a href="https://arxiv.org/abs/2307.05908">&ldquo;Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding&rdquo;</a> arXiv:2307.05908 [cs.CL]</p>
<p>[10] Zhang et al. <a href="https://arxiv.org/abs/2309.08168">&ldquo;Draft &amp; Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding&rdquo;</a> arXiv:2309.08168 [cs.CL]</p>
<p>[11] Santilli et al. <a href="https://aclanthology.org/2023.acl-long.689/">&ldquo;Accelerating Transformer Inference for Translation via Parallel Decoding&rdquo;</a> In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12336–12355, Toronto, Canada. Association for Computational Linguistics.</p>
<p>[12] Fu et al. <a href="https://arxiv.org/abs/2402.02057">&ldquo;Break the Sequential Dependency of LLM Inference Using Lookahead Decoding&rdquo;</a> arXiv:2402.02057 [cs.LG]</p>
<p>[13] Monea et al. <a href="https://arxiv.org/abs/2311.13581">&ldquo;PaSS: Parallel Speculative Sampling&rdquo;</a> arXiv:2311.13581 [cs.CL]</p>
<p>[14] Hopper et al. <a href="https://arxiv.org/abs/2310.12072">&ldquo;SPEED: Speculative Pipelined Execution for Efficient Decoding&rdquo;</a> arXiv:2310.12072 [cs.CL]</p>
<p>[15] Xia et al. <a href="https://arxiv.org/abs/2203.16487">&ldquo;Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation&rdquo;</a> arXiv:2203.16487 [cs.CL]</p>
<p>[16] Leviathan et al. <a href="https://proceedings.mlr.press/v202/leviathan23a.html">&ldquo;Fast Inference from Transformers via Speculative Decoding&rdquo;</a> Proceedings of the 40th International Conference on Machine Learning, PMLR 202:19274-19286, 2023.</p>
<p>[17] Chen et al. <a href="https://arxiv.org/abs/2302.01318">&ldquo;Accelerating Large Language Model Decoding with Speculative Sampling&rdquo;</a> arXiv:2302.01318 [cs.CL]</p>
<p>[18] Zhou et al. <a href="https://arxiv.org/abs/2310.08461">&ldquo;DistillSpec: Improving Speculative Decoding via Knowledge Distillation&rdquo;</a> arXiv:2310.08461 [cs.CL]</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://linshuhuai.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://linshuhuai.github.io/tags/ml-systems/">ML Systems</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://linshuhuai.github.io/posts/how-to-build-machine-learning-applications/">
    <span class="title">« Prev</span>
    <br>
    <span>How to Build Machine Learning Applications</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative Decoding on x"
            href="https://x.com/intent/tweet/?text=Speculative%20Decoding&amp;url=https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f&amp;hashtags=LLM%2cMLSystems">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative Decoding on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f&amp;title=Speculative%20Decoding&amp;summary=Speculative%20Decoding&amp;source=https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative Decoding on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f&title=Speculative%20Decoding">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative Decoding on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative Decoding on whatsapp"
            href="https://api.whatsapp.com/send?text=Speculative%20Decoding%20-%20https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative Decoding on telegram"
            href="https://telegram.me/share/url?text=Speculative%20Decoding&amp;url=https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Speculative Decoding on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Speculative%20Decoding&u=https%3a%2f%2flinshuhuai.github.io%2fposts%2fspeculative-decoding%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://linshuhuai.github.io/">Shuhuai&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
